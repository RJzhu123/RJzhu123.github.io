---
layout:     post                    # 使用的布局（不需要改）
title:      pos词性标注 tagging入门               # 标题 
subtitle:   nlp664-week5 #副标题
date:       2020-02-10              # 时间
author:     renjie                      # 作者
header-img: img/aurora.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - nlp
---
<font size="4"></font><br />

## Part-of-Speech(POS) tagging入门
> nlp664-week5

pos，词性标注。对文中的单词进行词性标注，可以给我们这个单词及相邻单词的信息。

英文词性有8种基本形式，这里简单把课上ppt的内容截出来
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrubtiyjjj31c40pegrc.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbruc9gidfj314n0u07b9.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrucnv4ibj315a0u0n2v.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrucysthbj315d0u00xq.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrudxqp8hj31640u079m.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbruenn6l2j319a0tiaf0.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbruetoiv4j311d0u0gq2.jpg)
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbruf31v9rj313v0u0jwf.jpg)
> 像是高中英语语法课。。。

***

进入正题：POS
### 三种常用的pos文本库
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrui0et05j31840pigpz.jpg)
> 课程中主要用penn treebank, 具体45个tags可以从这个网站里查到[treebank_tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)

> penn treebank主要用斜杠/，斜杠/后为tag

单词的多义性，语言的灵活性导致了pos的困难
### 主要几种pos算法
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrumxwxgaj317n0u0jyq.jpg)
简单的说，一种为算文本库中单词属于这个tag的概率，选概率最大的，另外一种则看words的feature来判断，类machine learning里面的classification

### 1.N-gram Approach
>N-gram taggers: uses the context of (a few) previous tags

计算一串单词序列归于一串序列的概率，选择最高的那个。  
所谓uni-gram, bi-gram, tri-gram则表示的是序列的长度（1，2，3）
> 举例：This is a round table.   

计算round的tag，拿bigram举例，就是取“a round"，a的tag为DT (determiner)。 然后算文本库（treebank）里 DT后的round的各种标签的概率（形容词还是名词），若是DT后跟形容词的概率高，那么就把这句话里的round归为形容词JJ, 反之亦然。

### 2.HMM Approach
>Hidden Markov Model (HMM) taggers: uses the context of the entire
sequence of words and previous tags

比ngram更复杂的概率模型。就是找出概率最大的标签组。

<center><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gbrv0zdp90j30pk06g0vb.jpg" width="40%"/></center>
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrv3e6ktwj31880rin26.jpg)
根据贝叶斯定律，且文本的排列组合概率相同，可以如上图化简
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrv5azq12j315q0r079f.jpg)
**likelihood: 假设某个单词的概率仅取决于tag。**
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrv9owl0zj317i0ow79l.jpg)
**prior：利用bigram假设某个tag的概率仅取决于前面的tags。**
![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrv9uewttj318a0sqgrh.jpg)

>举例：Bolt/NNP is/VBZ expected/VBN to/TO **race/VB** tomorrow/NR  

>&#160;Bolt/NNP is/VBZ expected/VBN to/TO **race/NN** tomorrow/NR

如何确定race是VB还是NN呢，首先计算likelihood，就是NN标签的情况下是这个单词race的概率。 然后计算prior，就是分别计算VB和NN前面是TO的概率和后面是NR的概率，相乘即可。 原理是这两个句子只有一个地方不同，所以可以从公式化简成这样。

## 3.Feature-based Classifiers
>A feature-based classifier is an algorithm that will take a word and
assign a POS tag based on features of the word in its context in the
sentence.

Naïve Bayes, Maximum Entropy (MaxEnt), Support Vector Machines (SVM)都有用到这些传统的分类器.

### Features of words
- Word
- Prefixes
- ­Suffixes
- Capitalization 
- Word shapes
> 注意，features不包括tags，和之前的ngram和hmm算法不一样

### 算法使用步骤
训练集 -> 发现featrues -> 导入分类器classifier -> featrues和classifier导入测试集 -> 和gold standard比较（也就是人工贴标签，98%的准确率） -> 调试之前的模型训练 -> 重复测试

![](https://tva1.sinaimg.cn/large/0082zybpgy1gbrvsf4mf5j31bx0u07ao.jpg)

## 小结

- 除了传统标准文本的pos，现在还有针对社交媒体的文本的pos
- pos应用在了很多现实生活中的nlp系统，作为预处理模块
- pos算法有更高level更广泛的应用