---
layout:     post                    # 使用的布局（不需要改）
title:      李宏毅 机器学习（1）- 梯度下降、朴素贝叶斯、线性回归、逻辑回归     # 标题 
subtitle:    机器学习#副标题
date:       2020-05-26              # 时间
author:     renjie                      # 作者
header-img: img/aurora.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 李宏毅网课
---
<font size="4"></font><br />
## 李宏毅 机器学习（1）- 梯度下降、朴素贝叶斯、线性回归、逻辑回归

## Lecture 1
### Regularization:
 让曲线更平滑，增加training error 但会减少test error.
 
> regularization 可以让曲线平滑，降低variance

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf3994v40dj31760hggxn.jpg)

### Adagrad
AdaGrad的基本思想是对每个变量用不同的学习率，这个学习率在一开始比较大，用于快速梯度下降。随着优化过程的进行，对于已经下降很多的变量，则减缓学习率，对于还没怎么下降的变量，则保持一个较大的学习率:
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf39k36nxxj31nc0f2wio.jpg)


## Lecture 2
bias  variance, 718 讲过，略

## Lecture 3 gradient descent
### 1:如何确定learningrate
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf5piqlxidj314f0u01kx.jpg)
画learning rate和loss的图确定learning rate

### Adagrad方法
或者设置可调节的learning rate
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf5plm5dwvj318k0se4qp.jpg)

adagrad
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf5ppc0sgnj313f0u01kx.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf5prn94npj31400u0avu.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6dniko76j315e0q6k9d.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6dhepipaj31830u0npd.jpg)
> 可以看出 , 如果跨参数，那么微分值越大不一定是离最低点越远，例如c的微分值大，a的微分值小，但是c距离最低点的距离比a大

> 所以最好的step还和二次微分有关，最佳的一步是一次微分的绝对值/二次微分

> 那式子中为什么不采用二次微分，而是之前微分平方和的根号呢，是因为二次微分计算量复杂，而用根号平方和的方法可以预估二次微分（如果数据点多的话）

ada系列的，**adam**最稳定

### 2:快速方法：stochastic gradient descent 随机梯度下降

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6dq8ha3bj317j0u07u6.jpg)
相比于普通的gradient descent （用所有的sample 更新参数）， stochastic gradient descent采用的是每次随机取样，根据那单个样品更新参数
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6dsmoweyj31a60u0kjl.jpg)
相比于批量梯度，这样的方法更快，更快收敛，虽然不是全局最优，但很多时候是我们可以接受的

### 3: Feature Scaling
make different features have the same scaling

(scaling不一样，变动一点点w对某个feature可能对结果影响很大）
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6dyxkamdj31560u01kx.jpg)

常用方法：标准化，减去mean然后除std
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6e04smkpj31860u01kx.jpg)

### 4. Gradient Descent 原理
泰勒级数
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6e4la927j31940u0h3f.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6e69asljj319x0u01c7.jpg)

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6e7o16k8j318h0u04qp.jpg)
问题转换为
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6eahbautj315i0u0txx.jpg)
得出结果
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6ebjxf2lj319q0u07wh.jpg)
因为让上式子成立，red circle要很小，也就是说learning rate必须足够小

考虑二次微分会更精确，但是在deep learning里我们无法承受这样大的计算量

### 5.Gradient Descent 限制
第一： local minimum，local minimum，local minimum！！！

第二：会卡在saddle point鞍点，微分值为0

**最最容易遇到的是：在plateau上卡住了，特别慢**
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6ehdc94xj315c0u0hdt.jpg)

## Lecture 4 Classification 
如果用regression来硬做，可能会在decrese error的过程中，分错类
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6eumzhogj31390u07wh.jpg)
### 1. 朴素贝叶斯
**朴素贝叶斯**：先算 prior 概率，假设样品概率分布为正态分布
> 何为朴素naive：假设变量都是independent的
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6ff3u6oej316f0u0b29.jpg)

 那怎么得到这个distribution：** maximum likelihood 极大似然估计**
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6fjzi46mj31550u0e81.jpg)
找出一个分布，让sample点出现的概率最大，也就是找出这个likelihood 的function的最大值
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6fmbelalj315y0u0azl.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6folenfsj315y0u0hcr.jpg)

为了避免overfitting，减少参数，可以让每个class共享一个 covariance matrix
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6g80xfwbj314o0u01kx.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6gamcx8qj31d80rc1kx.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6geez4kdj316a0u07vd.jpg)

### 2.数学部分
后验概率
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6ggd63mgj317t0u0wv6.jpg)
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6gjitug8j31790u0qmy.jpg)
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6gkg0imlj31de0kwh2u.jpg)
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6glosyz9j31c60o24in.jpg)
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6go1497cj31by0skncx.jpg)
 这就是为什么boundary为线性的?
 
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6golqcgcj31aw06g0zi.jpg)
 在这里咱们可以看到，我们费劲周折假设分布，估计这些参数，极大似然估计法等等，就是为了得出w和b，那么我们为什么不直接来算w和b呢？直接算的方法就是logistic regression
 

 
 
## Lecture 5: Logistic Regression
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7ssk6an4j31bw0u0ts9.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7sv9st35j316c0smx2q.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7szczb3pj313s0u07wh.jpg)
> cross entropy 两个distribution有多接近

然后gradient descent就好啦
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7t6cvxkaj318c0u07rl.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7t7ra2a7j31910u07um.jpg)

为什么不能用最小二乘法的解释：
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7untzfvxj316e0u0azp.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7unz74lhj319l0u07wh.jpg)


### 1、 multiclass
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7vk7g9arj315l0u04qp.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7w6lmq2qj31650u0kcz.jpg)


### 2. 线性回归和逻辑回归的区别：
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7t1a8dmbj31630u04qp.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7u1rv9ngj31bc0agn52.jpg)

> 神奇的是两者的更新function一样

** 逻辑回归得到的是判别模型，朴素贝叶斯得到的则是生成模型**
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6gy6twlyj310u0do0wf.jpg)
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6gwfa01aj30vh0m6ac9.jpg)
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7ut2f7moj316q0u0x5t.jpg)
 > discriminative 一般表现比 generative好
 
 ![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf6gsjpvzgj311y0ty7bf.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7v04zdwjj31880n0b17.jpg)
> 数据量少的时候，assume一个分布可能会有优势，数据量多但是noise多的dataset，generative也能有优势
> 
> 语音识别是discriminate的model，也就是dnn，但是语音识别的系统是generative的，因为这个系统要先有prior概率，也就是某句话说出来的频率，language model。

### 3、 逻辑回归的限制
如果feature space是非线性的，那很难做分类
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7wk1zbyrj31a10u0kjl.jpg)

解决办法就是feature transformation， 例如可以把feature转换为到某一个点的距离。

现在的问题是：**feature transformation 很难做，很难找出这样的办法**

那现在我们可以用串联逻辑分类器的方法进行featrue transformation


![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf7wns2vrdj31bk0p87jp.jpg)

这！ 就是神经网络！！！
下篇请见 deep learning 部分
