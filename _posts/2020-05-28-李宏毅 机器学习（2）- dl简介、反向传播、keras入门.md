---
layout:     post                    # 使用的布局（不需要改）
title:      李宏毅 机器学习（2）- dl简介、反向传播、keras入门     # 标题 
subtitle:    机器学习        				#副标题
date:       2020-05-27              # 时间
author:     renjie                      # 作者
header-img: img/aurora.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 李宏毅网课
---
<font size="4"></font><br />
## 李宏毅 机器学习（2）- dl简介、反向传播、keras入门

## 1、历史
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8isv1plmj315y0u07wh.jpg)

## 2、理解neural network
### step1: define function set
尽管neural network很复杂，但是可以把它看成是一个function，进去input 是一个vector，出来output也是一个vector。定义神经网络结构就是定义这个function set

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8ivqpz9pj316k0u01kr.jpg)

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8ixrcughj314m0ow1fh.jpg)

**矩阵运算**
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8j0cuk5wj31a20t6nem.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8j1s3i0gj318i0u07wh.jpg)

**Hidden layer实际上就是做的feature extractor replacing 和 feature engineering的工作**

**经过softmax处理后，outputlayer就是multiclassifier**
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8j4o5eq6j318o0tkb21.jpg)

在训练之前，关键就是设计神经网络的结构： trial and error + intuition 直觉经验
### step2: goodness of function and step3: find best function

loss function = cross entropy
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8jd1im1ej316v0u0kh5.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8jdksjzhj314p0u07wh.jpg)

还是用gradient descent 进行reduce loss 

## 3、backpropagation
> Backpropagation(反向传播)，就是告诉我们用gradient descent来train一个neural network的时候该怎么做，它只是求微分的一种方法，而不是一种新的算法

### Chain Rule(链式法则)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8jmkiwk8j31010u0dqn.jpg)

> 有人已经做了详细的笔记，推导部分见[https://sakura-gh.github.io/ML-notes/ML-notes-html/9_Backpropagation.html](https://sakura-gh.github.io/ML-notes/ML-notes-html/9_Backpropagation.html)

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8kb8aodpj311n0r6jud.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8kapocjrj31180sbmzn.jpg)
## summary
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8k28sjwpj315j0u0e5b.jpg)

Forward Pass + Backward Pass

## 4.Keras
keras 是tensorflow和theano的一个interface，简单易学

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8q9nub81j313g0trn29.jpg)
> 建立model的过程比较像搭积木

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8q9qu09cj311x0so0zy.jpg)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8q9w27fej31430stq8t.jpg)

### mini batch vs stochastic gradient descent
mini batch ，假设我们设size为100，那么就是随机抽100个数据放入一个batch，每个batch更新一次参数，epoch数就是对所有batch更新几轮。
> batch size =1 , 则变为stochastic gradient descent

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8qa0es7cj315k0tun15.jpg)

### Batch size and Training Speed
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8qa3rzjbj31330sg42t.jpg)

相较于batch size=1，你会更倾向于选batch size=10，因为batch size=10的时候，是会比较稳定的，**因为由更大的数据集计算的梯度能够更好的代表样本总体，从而更准确的朝向极值所在的方向**


**batch size会受到GPU平行加速的限制，太大可能导致在train的时候卡住，而且容易掉到local minimum里去**
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8qj0sgefj30z50u0tqj.jpg)

因此batch size既不能太大，因为它会受到硬件GPU平行加速的限制，导致update次数过于缓慢，并且由于缺少随机性而很容易在梯度下降的过程中卡在saddle point或是local minima的地方(极端情况是Full batch)；而且batch size也不能太小，因为它会导致速度优势不明显的情况下，梯度下降曲线过于不稳定，算法可能永远也不会收敛(极端情况是Stochastic gradient descent)
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gf8qlbrpxzj310p0rvtb1.jpg)

### code 
~~~python
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Activation, Dense

# step 1：define a set of function——neural network

model = Sequential()

model.add(Dense(input_dim=28 * 28, units=500, activation='sigmoid'))

model.add(Dense(units=500, activation='sigmoid'))

model.add(Dense(units=10, activation='softmax'))

# Step 2：goodness of function——cross entropy

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Step 3：pick the best function


(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

model.fit(x_train,y_train,batch_size=100,epochs=20)
score = model.evaluate(x_test,y_test)
print('Total loss on Testing Set:',score[0])
print('Accuracy of Testing Set:',score[1])
~~~
~~~python
Total loss on Testing Set: 0.11766348047330975
Accuracy of Testing Set: 0.9623000025749207
~~~