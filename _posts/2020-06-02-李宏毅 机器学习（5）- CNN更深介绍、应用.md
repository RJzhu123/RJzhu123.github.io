---
layout:     post                    # 使用的布局（不需要改）
title:      李宏毅 机器学习（5）- CNN更深介绍、应用     # 标题 
subtitle:    深度学习        				#副标题
date:       2020-06-02             # 时间
author:     renjie                      # 作者
header-img: img/aurora.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 李宏毅网课
---
<font size="4"></font><br />
## 李宏毅 机器学习（5）- CNN更深介绍、应用

## 1、What does CNN do？

### 第一层convolution
还是上个例子
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gfbf835wqnj313d0r9wj3.jpg)

要分析第一个convolution的filter是比较容易的，因为第一个convolution layer里面，每一个filter就是一个3\*3的matrix，它对应到3\*3范围内的9个pixel，所以你只要看这个filter的值，就可以知道它在detect什么东西，因此第一层的filter是很容易理解的
但是你比较没有办法想像它在做什么事情的，是第二层的filter，它们是50个同样为3\*3的filter，但是这些filter的input并不是pixel，而是做完convolution再做Max pooling的结果，因此filter考虑的范围并不是3\*3=9个pixel，而是一个长宽为3\*3，高为25的cubic，filter实际在image上看到的范围是远大于9个pixel的，所以你就算把它的weight拿出来，也不知道它在做什么

### 第二层convolution
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffw2rdr0ij311e0stwie.jpg)
想找出第二层的第k个filter在干嘛，就是让第k个激活程度最大。也就是定好了参数，反过来做gradient
 ascent 来找出激活程度最大的input x。
 
 仔细一想这个方法还是颇为神妙的，因为我们现在是把input x作为要找的参数，对它去用gradient descent或ascent进行update，原来在train CNN的时候，input是固定的，model的参数是要用gradient descent去找出来的；但是现在这个立场是反过来的，在这个task里面model的参数是固定的，我们要用gradient ascent去update这个x，让它可以使degree of activation最大
 
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffw71vhilj311e0ssjuw.jpg)

上图就是得到的结果，每个filter的工作就是去detect某一种pattern，detect某一种线条，上图所示的filter所detect的就是不同角度的线条，所以今天input有不同线条的话，某一个filter会去找到让它兴奋度最高的匹配对象，这个时候它的output就是最大的。

### 后面的neuron做什么
我们做完convolution和max pooling之后，会将结果用Flatten展开，然后丢到Fully connected的neural network里面去，之前已经搞清楚了filter是做什么的，那我们也想要知道在这个neural network里的每一个neuron是做什么的，所以就对刚才的做法如法炮制
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffw9y6zjnj31130swwin.jpg)

这9张图跟之前filter所观察到的情形是很不一样的，刚才我们观察到的是类似纹路的东西，那是因为每个filter考虑的只是图上一部分的vision，所以它detect的是一种texture；但是在做完Flatten以后，每一个neuron不再是只看整张图的一小部分，它现在的工作是看整张图，所以对每一个neuron来说，让它最兴奋的、activation最大的image，不再是texture，而是一个完整的图形
 
### output
 
MNIST的output就是10维，我们把某一维拿出来，然后同样去找一张image x，使这个维度的output值最大，既然现在每一个output的每一个dimension就对应到一个数字，那如果我们去找一张image x，它可以让对应到数字1的那个output layer的neuron的output值最大，那这张image显然应该看起来会像是数字1，你甚至可以期待，搞不好用这个方法就可以让machine自动画出数字

但实际上，我们得到的结果是这样子，如下图所示
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwbcfnfqj31130t1aeu.jpg)
今天这个neural network，它所学到的东西跟我们人类一般的想象认知是不一样的
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwczwqt7j31250qt76x.jpg)
加上l1的regularization以后，结果会好一些。


## 2、 cnn其他应用
### deep dream
引出了Deep Dream的概念，也就是说，如果你给machine一张image，它会在这个image里面加上它看到的东西

怎么做这件事情呢？你就找一张image丢到CNN里面去，然后你把某一个convolution layer里面的filter或是fully connected layer里的某一个hidden layer的output拿出来，它其实是一个vector；接下来把本来是positive的dimension值调大，negative的dimension值调小，也就是让正的更正，负的更负，然后把它作为新的image的目标
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwi059q5j30xm0p27wh.jpg)

这里就是把3.9、2.3的值调大，-1.5的值调小，总体来说就是使它们的绝对值变大，然后用gradient descent的方法找一张image x，让它通过这个hidden layer后的output就是你调整后的target，这么做的目的就是，**让CNN夸大化它看到的东西**——make CNN exaggerates what is sees
也就是说，如果某个filter有被activate，那你让它被activate的更剧烈，CNN可能本来看到了某一样东西，那现在你就让它看起来更像原来看到的东西，这就是所谓的夸大化

如果你把上面这张image拿去做Deep Dream的话，你看到的结果就会像下面这个样子
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwipqg1pj30u80oc7wh.jpg)

### deep style
Deep Dream还有一个进阶的版本，就叫做Deep Style，如果今天你input一张image，Deep Style做的事情就是让machine去修改这张图，让它有另外一张图的风格，如下所示

> 参考paper：[**https://arxiv.org/abs/1508.06576**](https://arxiv.org/abs/1508.06576)

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwmcv4uhj30yi0o2h7x.jpg)
这里仅讲述Deep Style的大致思路，你把原来的image丢给CNN，得到CNN filter的output，代表这样image里面有什么样的content，然后你把呐喊这张图也丢到CNN里面得到filter的output，注意，我们并不在于一个filter output的value到底是什么，一个单独的数字并不能代表任何的问题，我们真正在意的是，filter和filter的output之间的correlation，这个correlation代表了一张image的style

接下来你就再用一个CNN去找一张image，**这张image的content像左边的图片**，比如这张image的filter output的value像左边的图片；同时让**这张image的style像右边的图片**，所谓的style像右边的图片是说，这张image output的filter之间的correlation像右边这张图片
最终你用gradient descent找到一张image，同时可以maximize左边的content和右边的style，它的样子就像上图左下角所示

### Playing GO
#### 为什么cnn可以得到更好的performance在下围棋这件事情上呢？

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwtkb7y6j310m0qwdn7.jpg)

我们之前举的例子都是把CNN用在图像上面，也就是input是一个matrix，而棋盘其实可以很自然地表示成一个19*19的matrix，那对CNN来说，就是直接把它当成一个image来看待，然后再output下一步要落子的位置，具体的training process是这样的:

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwu5yycbj311w0rd4az.jpg)
你就搜集很多棋谱，比如说上图这个是进藤光和社青春的棋谱，初手下在5之五，次手下在天元，然后再下在5之五，接下来你就告诉machine说，看到落子在5之五，CNN的output就是天元的地方是1，其他的output是0；看到5之五和天元都有子，那你的output就是5之五的地方是1，其他都是0
上面是supervised的部分，那其实呢AlphaGo还有reinforcement learning的部分，这个后面的章节会讲到

#### 为什么想到用cnn下围棋

还是那三个特性：

- Some patterns are much smaller than the whole image
- The same patterns appear in different regions
- Subsampling the pixels will not change the object

在property 1，有一些pattern是比整张image要小得多，在围棋上，可能也有同样的现象，比如下图中一个白子被3个黑子围住，这个叫做吃，如果下一个黑子落在白子下面，就可以把白子提走；只有另一个白子接在下面，它才不会被提走

那现在你只需要看这个小小的范围，就可以侦测这个白子是不是属于被叫吃的状态，你不需要看整个棋盘，才知道这件事情，所以这件事情跟image有着同样的性质；在AlphaGo里面，它第一个layer其实就是用5\*5的filter，显然做这个设计的人，觉得围棋上最基本的pattern可能都是在5\*5的范围内就可以被侦测出来

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwwigti1j30zy0re7e3.jpg)
 
 在property 2，同样的pattern可能会出现在不同的region，在围棋上也可能有这个现象，像这个叫吃的pattern，它可以出现在棋盘的左上角，也可以出现在右下角，它们都是叫吃，都代表了同样的意义，所以你可以用同一个detector，来处理这些在不同位置的同样的pattern
所以对围棋来说呢，它在第一个observation和第二个observation是有这个image的特性的，但是，让我们没有办法想通的地方，就是第三点

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffwz7ql8sj31560u0k8d.jpg)

它是这样说的，input是一个19\*19\*48的image，其中19*19是棋盘的格局，对Alpha来说，每一个位置都用48个value来描述，这是因为加上了domain knowledge，它不只是描述某位置有没有白子或黑子，它还会观察这个位置是不是处于叫吃的状态等等

先用一个hidden layer对image做zero padding，也就是把原来19\*19的image外围补0，让它变成一张23\*23的image，然后使用k个5\*5的filter对该image做convolution，stride设为1，activation function用的是ReLU，得到的output是21\*21的image；接下来使用k个3*3的filter，stride设为1，activation function还是使用ReLU，...

你会发现这个AlphaGo的network structure一直在用convolution，其实根本就没有使用Max Pooling，原因并不是疏失了什么之类的，而是根据围棋的特性，我们本来就不需要在围棋的CNN里面，用Max pooling这样的构架


**所以设计cnn可以有创新之处的**


### Speech、Text

#### Speech

CNN也可以用在很多其他的task里面，比如语音处理上，我们可以把一段声音表示成spectrogram，**spectrogram的横轴是时间，纵轴则是这一段时间里声音的频率**
下图中是一段“你好”的音频，偏红色代表这段时间里该频率的energy是比较大的，也就对应着“你”和“好”这两个字，也就是说spectrogram用颜色来描述某一个时刻不同频率的能量

我们也可以让机器把这个spectrogram就当作一张image，然后用CNN来判断说，input的这张image对应着什么样的声音信号，那通常用来判断结果的单位，比如phoneme，就是类似音标这样的单位
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffx2jkxcsj30zy0sxgsp.jpg)

这边比较神奇的地方就是，当我们把一段spectrogram当作image丢到CNN里面的时候，在语音上，我们通常只考虑在frequency(频率)方向上移动的filter，我们的filter就像上图这样，是长方形的，它的宽就跟image的宽是一样的，并且**filter只在Frequency即纵坐标的方向上移动，而不在时间的序列上移动**
这是因为在语音里面，CNN的output后面都还会再接别的东西，比如接LSTM之类，它们都已经有考虑typical的information，所以你在CNN里面再考虑一次时间的information其实没有什么特别的帮助，但是为什么在频率上 的filter有帮助呢？

我们用CNN的目的是为了用同一个filter把相同的pattern给detect出来，在声音讯号上，虽然男生和女生说同样的话看起来这个spectrogram是非常不一样的，但实际上他们的不同只是表现在一个频率的shift而已(整体在频率上的位移)，男生说的“你好”跟女生说的“你好”，它们的pattern其实是一样的，比如pattern是spectrogram变化的情形，男生女生的声音的变化情况可能是一样的，它们的差别可能只是所在的频率范围不同而已，所以filter在frequency的direction上移动是有效的

所以，这又是另外一个例子，当你把CNN用在一个Application的时候呢，你永远要想一想这个Application的特性是什么，根据这个特性你再去design network的structure，才会真正在理解的基础上去解决问题

#### Text
CNN也可以用在文字处理上，假设你的input是一个word sequence，你要做的事情是让machine侦测这个word sequence代表的意思是positive的还是negative的

首先你把这个word sequence里面的每一个word都用一个vector来表示，vector代表的这个word本身的semantic (语义)，那如果两个word本身含义越接近的话，它们的vector在高维的空间上就越接近，这个东西就叫做word embedding

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gffx6fp23kj311q0rqwli.jpg)

把一个sentence里面所有word的vector排在一起，它就变成了一张image，你把CNN套用到这个image上，那filter的样子就是上图蓝色的matrix，它的高和image的高是一样的，然后把filter沿着句子里词汇的顺序来移动，每个filter移动完成之后都会得到一个由内积结果组成的vector，不同的filter就会得到不同的vector，接下来做Max pooling，然后把Max pooling的结果丢到fully connected layer里面，你就会得到最后的output

**与语音处理不同的是，在文字处理上，filter只在时间的序列(按照word的顺序)上移动，而不在这个embedding的dimension上移动；因为在word embedding里面，不同dimension是independent的，它们是相互独立的，不会出现有两个相同的pattern的情况，所以在这个方向上面移动filter，是没有意义的**


所以这又是另外一个例子，虽然大家觉得CNN很powerful，你可以用在各个不同的地方，但是当你应用到一个新的task的时候，你要想一想这个新的task在设计CNN的构架的时候，到底该怎么做

## 3、总结

### 三个property
- Some patterns are much smaller than the whole image ——property 1
- The same patterns appear in different regions ——property 2
- Subsampling the pixels will not change the object ——property 3

### 两个架构

- convolution架构：针对property 1和property 2

- max pooling架构：针对property 3

### 一个理念 
针对不同的application要设计符合它特性的network structure，而不是生硬套用，这就是CNN架构的设计理念：

**应用之道，存乎一心**
